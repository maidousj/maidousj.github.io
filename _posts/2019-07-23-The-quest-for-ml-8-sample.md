---
title: The quest for ml 8 sample
layout: post
date: 2019-07-23 16:53
image: /assets/images/
headerImage: false
category: blog
tag:
- machine learning
- questions
- sample
author: Sun
---

Q1: 采样在机器学习中的应用？

A1: 采样本质是对随机现象的模拟，即根据给定的概率分布，模拟产生对应的随机事件。

​	采样也可以看作是一种非参数模型，即用较少量的样本点来近似总体分布，并刻画分布中的不确定性。这个角度考虑，也可以当作一种信息降维。

​	常见的如自助法和刀切法(Jack knife)，通过对样本的多次重采样来估计统计量的偏差、方差等信息。重采样来处理分类模型的训练样本不均衡问题。

​	此外，很多模型由于结构复杂、含有隐变量等原因，没有显示的解析解，可以利用采样方法进行随机模拟，对复杂模型进行近似求解或推理。例如，在隐狄利克雷模型和深度玻尔兹曼机(Deep Boltzmann Machines, DBM)的求解过程中，可以采用**吉布斯采样**来简化求解过程。

Q1-1: 如何用自助法或刀切法来估计偏差、方差？



Q1-2: 在隐狄利克雷模型和深度玻尔兹曼机中，具体如何用吉布斯采样求解？



Q1-3: 进行模型求解时，马尔可夫蒙特卡洛采样法与常见的最大期望算法、变分推断法有什么联系和区别？



Q2: 如何编程实现均匀分布的随机数生成器？

A2: 首先，计算机只能产生伪随机数（伪随机是指这些数字虽然是通过确定性的程序产生的，但是它们能通过近似的随机性测试）。其次，计算机只能产生离散的均匀分布（因为计算机的存储和计算单元只能处理离散状态值），然后通过离散分布来逼近连续分布。

​	一般用线性同余法(Linear Congruential Generator)来生成离散均匀分布伪随机数，计算公式为

$$ x_{t+1} = a \cdot x_t + c (\mod m) \tag{1}$$,

产生区间$[0, m-1]$上的随机整数。但是，上式得到的随机数显然不是相互独立的。事实上，该算法需要$a$和$m$的精心选择才能使循环周期尽可能接近$m$。具体实现中，比如gcc采用的glibc版本：$m = 2^{31}-1, a = 1103515245, c = 12345$。

Q2-1: 线性同余法的随机种子($x_0$)一般如何选定？



Q2-2: 如果需要产生高维样本或大量样本，线性同余法存在什么问题？



Q2-3: 如何证明上述得到的序列可以近似为均匀分布?



Q3: 通用的采样方法或采样策略？



